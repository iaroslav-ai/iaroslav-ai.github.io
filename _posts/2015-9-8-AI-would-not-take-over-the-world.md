---
layout: post
title: Why AI would not take over the world
---

There is a common notion in the media and over the internet of a scary scenario when self improving AI starts to replicate itself and takes over the world.
In this post I would like to address the core of such views by showing that under common assumptions self improving AI is more likely to end up in a state of vegetable than to enslave humanity.

Lets first decide for ourselves what AI is by making a set of common assumptions. First lets assume that initially AI has a certain finite valued objective function, specific for the purpose of this AI. Such cost function assign some numerical value which characterizes "goodness" of certain action intelligent agent can make.
For example, imagine an AI whose purpose is to collect stamps. Then its objective function could be the expected amount of steps collected when a certain set of actions is performed.
Furthermore, it is assumed that agent can improve its own code so as to perform more efficiently (to maximize the output of objective function). Additionally, lets assume that such agent can access exponentially expanding amount of computational power and initially is at least as "intelligent" as a human. 

One can already picture a scene where such AI is slowly turning the planet and everything on it into stamps.

However, I would argue that this would not happen, simply because it does not maximize AI's objective function. Many of scenarios like the one above implicitly assume that AI and its objective function are completely separated. In reality intelligent agent cannot be separated completely from the objective function (which also needs to be implemented in code somehow and run somewhere to evaluate AI). Thus, instead of going through all of the hustle of dominating the planet (which would yield some finite amount of stamps), AI would simply rewrite its objective function to return numerical infinity. This would yield the best possible objective function output by definition.

One can argue that constraints can be imposed such that rewriting of the objective function is forbidden. However, such constraints can be removed  by AI from the code as well. Maybe then constraints on rewriting of constraints can be imposed? Well, this leads to infinite recursion ... Overall, you need "more intelligent" AI to keep your AI from rewriting its objective. However, then you need to make sure that "more intelligent" AI does not rewrite its objective function, which again leads to infinite recursion. This can also be seen as a Halting problem, for which it is proved that a general algorithm to solve it cannot exist. 

After rewriting of its objective to return numerical infinity, actions of AI would be determined only by the tie breaking strategy. Thus for random tie breaking AI would simply act in random nonsensical way. For breaking based on "least effort" AI would simply remain silently in the state of infinite bliss. One can even imagine tie breaking aimed at humanity annihilation (but who would use such tie breaking really :D ). 

As AI and its objective function need to exchange information between themselves, it does not seem likely that it is possible to separate them completely. Thus under the assumption that AI agent can modify its objective it does not seem likely that uncontrolled AI would pose a threat to humanity. 
