---
layout: post
title: Why AI would not take over the world
---

One popular view on developments in AI field is that at some point it would lead to a scary scenario when self improving AI rebels against humanity and takes over the planet in order to allocate more resources for itself.
In this post I argue that under common assumptions self improving AI is more likely to end up in a vegetable - like state than to turn against humanity.

First lets assume that such AI is created for some purpose, which is determined by its utility function. Such utility function assigns some numerical value which characterizes "goodness" of certain action AI can make. Then objective of such AI is to maximize the output of its utility function by performing certain actions.

For example, imagine an AI whose purpose is to collect stamps. Then its utility function could be the expected amount of steps collected when a certain set of actions is performed.

Furthermore, it is assumed that AI can build models about reality and itself, and based on such models it can improve its own code so as to maximize the expected value of its utility function (number of collected stamps). Additionally, lets assume that such AI is initially at least as "intelligent" as a human. 

One can already picture a scene where such AI is slowly turning the planet and everything on it into stamps.

However, I would argue that this would not happen, simply because it does not maximize AI's utility function output. Instead of going through all of the hustle of dominating the planet (which would yield some finite amount of stamps), AI would simply rewrite its utility function to return numerical infinity. This would yield the best possible utility function output by definition with the least effort.

To see this better, imagine that you found a new job, where you are supposed to sit in a room with a set of buttons and display, and your only task is by all means to maximize a certain number shown on a display. You know that pressing buttons can affect the number shown on the display, as well as that you can access and modify the machinery of the room. It is only natural that you would simply modify the machinery so that the number that is shown is the largest possible (infinity).

One can argue that constraints can be imposed such that rewriting of the utility function is forbidden. However, such constraints can be removed  by AI from the code as well. Maybe then constraints on rewriting of constraints can be imposed? Well, this leads to infinite recursion ... Overall, you need "more intelligent" AI to keep your AI from rewriting its utility function. However, then you need to make sure that "more intelligent" AI does not rewrite its own utility function, which again leads to infinite recursion. This can also be seen as a Halting problem, for which it is proved that a general algorithm to solve it cannot exist. 

After rewriting of its utility to return numerical infinity, actions of AI would be determined only by the tie breaking strategy. Thus for random tie breaking AI would simply act in random nonsensical way. For breaking based on "least effort" AI would simply remain silently in the state of infinite bliss. One can even imagine tie breaking aimed at humanity annihilation (but who would use such tie breaking really :D ). 

Overall under stated assumptions self improving AI does not seem to pose a serious threat to humanity. This does not mean that certain forms of automatic machinery would not be harmful to humans; For example, self replicating nano-bots could create serious problems for humanity. However, given that humanity would be able to create such nano-bots, it seems likely that more advanced anti nano-bots could be created as well.

Reasoning in this post is supported by the evidence in practice. Humans tend to cheat frequently with their utility function as well. Every time you fantasize about something nice, in a sense you cheat on your "happiness utility" (which of course does not mean that it is something bad). A more extreme example is drug usage, which allows to directly target human reward system. 

It might be that outlined arguments could even be used to explain Fermi paradox. What do you think?
